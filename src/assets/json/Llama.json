{
    "nodes": {
        "LlamaForClassification": {"name": "LlamaForClassification"},
        "Model": {"name": "Model"},
        "LlamaModel": {"name": "LlamaModel"},
        "embed_tokens": {"name": "embed_tokens"},
        "Embedding": {"name": "Embedding"},
        "layers": {"name": "layers"},
        "ModuleList": {"name": "ModuleList"},
        "O-17": {"name": "O-17", "count": 18},
        "LlamaDecoderLayer": {"name": "LlamaDecoderLayer"},
        "self_attn": {"name": "self_attn"},
        "LlamaSdpaAttention": {"name": "LlamaSdpaAttention"},
        "q_proj": {"name": "q_proj"},
        "linear_q": {"name": "linear"},
        "k_proj": {"name": "k_proj"},
        "linear_k": {"name": "linear"},
        "v_proj": {"name": "v_proj"},
        "linear_v": {"name": "linear"},
        "o_proj": {"name": "o_proj"},
        "linear_o": {"name": "linear"},
        "rotary_emb": {"name": "rotary_emb"},
        "LlamaRotaryEmbedding": {"name": "LlamaRotaryEmbedding"},
        "mlp": {"name": "mlp"},
        "LlamaMLP": {"name": "LlamaMLP"},
        "gate_proj": {"name": "gate_proj"},
        "gate_linear": {"name": "linear"},
        "up_proj": {"name": "up_proj"},
        "up_proj_linear": {"name": "linear"},
        "down_proj": {"name": "down_proj"},
        "down_proj_linear": {"name": "linear"},
        "act_fn": {"name": "act_fn"},
        "PytorchGELUTanh": {"name": "PytorchGELUTanh"},
        "input_layernorm": {"name": "input_layernorm"},
        "LlamaRMSNorm2": {"name": "LlamaRMSNorm"},
        "post_attention_layernorm": {"name": "post_attention_layernorm"},
        "LlamaRMSNorm3": {"name": "LlamaRMSNorm"},
        "norm": {"name": "norm"},
        "LlamaRMSNorm": {"name": "LlamaRMSNorm"},
        "Score": {"name": "Score"},
        "linear": {"name": "linear"}
    },
    "edges": {
        "LlamaForClassification": {"source": "LlamaForClassification", "target": "Model"},
        "LlamaForClassification2": {"source": "LlamaForClassification", "target": "Score"},
        "Model": {"source": "Model", "target": "LlamaModel"},
        "LlamaModel": {"source": "LlamaModel", "target": "embed_tokens"},
        "LlamaMode2": {"source": "LlamaModel", "target": "layers"},
        "LlamaMode3": {"source": "LlamaModel", "target": "norm"},
        "embed_tokens": {"source": "embed_tokens", "target": "Embedding"},
        "layers": {"source": "layers", "target": "ModuleList"},
        "ModuleList": {"source": "ModuleList", "target": "O-17"},
        "O-17": {"source": "O-17", "target": "LlamaDecoderLayer"},
        "LlamaDecoderLayer": {"source": "LlamaDecoderLayer", "target": "self_attn"},
        "LlamaDecoderLayer2": {"source": "LlamaDecoderLayer", "target": "mlp"},
        "LlamaDecoderLayer3": {"source": "LlamaDecoderLayer", "target": "input_layernorm"},
        "LlamaDecoderLayer4": {"source": "LlamaDecoderLayer", "target": "post_attention_layernorm"},
        "self_attn": {"source": "self_attn", "target": "LlamaSdpaAttention"},
        "LlamaSdpaAttention": {"source": "LlamaSdpaAttention", "target": "q_proj"},
        "LlamaSdpaAttention2": {"source": "LlamaSdpaAttention", "target": "k_proj"},
        "LlamaSdpaAttention3": {"source": "LlamaSdpaAttention", "target": "v_proj"},
        "LlamaSdpaAttention4": {"source": "LlamaSdpaAttention", "target": "o_proj"},
        "LlamaSdpaAttention5": {"source": "LlamaSdpaAttention", "target": "rotary_emb"},
        "q_proj": {"source": "q_proj", "target": "linear_q"},
        "k_proj": {"source": "k_proj", "target": "linear_k"},
        "v_proj": {"source": "v_proj", "target": "linear_v"},
        "o_proj": {"source": "o_proj", "target": "linear_o"},
        "rotary_emb": {"source": "rotary_emb", "target": "LlamaRotaryEmbedding"},
        "mlp": {"source": "mlp", "target": "LlamaMLP"},
        "LlamaMLP": {"source": "LlamaMLP", "target": "gate_proj"},
        "LlamaMLP2": {"source": "LlamaMLP", "target": "up_proj"},
        "LlamaMLP3": {"source": "LlamaMLP", "target": "down_proj"},
        "LlamaMLP4": {"source": "LlamaMLP", "target": "act_fn"},
        "gate_proj": {"source": "gate_proj", "target": "gate_linear"},
        "up_proj": {"source": "up_proj", "target": "up_proj_linear"},
        "down_proj": {"source": "down_proj", "target": "down_proj_linear"},
        "act_fn": {"source": "act_fn", "target": "PytorchGELUTanh"},
        "input_layernorm": {"source": "input_layernorm", "target": "LlamaRMSNorm2"},
        "post_attention_layernorm": {"source": "post_attention_layernorm", "target": "LlamaRMSNorm3"},
        "norm": {"source": "norm", "target": "LlamaRMSNorm"},
        "Score": {"source": "Score", "target": "linear"}
    },
    "layouts": {
        "nodes": {
            "LlamaForClassification": { "x": 0, "y": 0 },
            "Model": { "x": 0, "y": 100 },
            "LlamaModel": { "x": 0, "y": 200 },
            "embed_tokens": { "x": -400, "y": 300 },
            "Embedding": { "x": -400, "y": 400 },
            "layers": { "x": 400, "y": 300 },
            "ModuleList": { "x": 400, "y": 400 },
            "O-17": { "x": 400, "y": 500 },
            "LlamaDecoderLayer": { "x": 400, "y": 600 },
            "self_attn": { "x": -200, "y": 700 },
            "LlamaSdpaAttention": { "x": -200, "y": 800 },
            "q_proj": { "x": -600, "y": 900 },
            "linear_q": { "x": -600, "y": 1000 },
            "k_proj": { "x": -200, "y": 900 },
            "linear_k": { "x": -200, "y": 1000 },
            "v_proj": { "x": 200, "y": 900 },
            "linear_v": { "x": 200, "y": 1000 },
            "o_proj": { "x": 600, "y": 900 },
            "linear_o": { "x": 600, "y": 1000 },
            "rotary_emb": { "x": -200, "y": 1100 },
            "LlamaRotaryEmbedding": { "x": -200, "y": 1200 },
            "mlp": { "x": 400, "y": 700 },
            "LlamaMLP": { "x": 400, "y": 800 },
            "gate_proj": { "x": 200, "y": 900 },
            "gate_linear": { "x": 200, "y": 1000 },
            "up_proj": { "x": 600, "y": 900 },
            "up_proj_linear": { "x": 600, "y": 1000 },
            "down_proj": { "x": 1000, "y": 900 },
            "down_proj_linear": { "x": 1000, "y": 1000 },
            "act_fn": { "x": 1400, "y": 900 },
            "PytorchGELUTanh": { "x": 1400, "y": 1000 },
            "input_layernorm": { "x": -600, "y": 700 },
            "LlamaRMSNorm2": { "x": -600, "y": 800 },
            "post_attention_layernorm": { "x": 0, "y": 700 },
            "LlamaRMSNorm3": { "x": 0, "y": 800 },
            "norm": { "x": 0, "y": 1300 },
            "LlamaRMSNorm": { "x": 0, "y": 1400 },
            "Score": { "x": -400, "y": 100 },
            "linear": { "x": -400, "y": 200 }
        }
    }
}
